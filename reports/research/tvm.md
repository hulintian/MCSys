# TVM: An Automated End-to-End Optimizing Compiler for Deep Learning 阅读
- 给不同的硬件后端，提供图层次的和算子层次的优化来加速
> 优化的本质是什么？
> 用一段“更好的”代码片段代替原来的。对计算的重新认识。

- 现有的DL框架如TF、PyTorch、MXNet等，基于图的优化，但是
  - 针对特定硬件
  - 高工程化
  - 厂商提供的算子库 -- 依赖人工、不透明，为了可移植性还需要避免使用新出的算子

- TVM，
  - 输入：高级的，特定的，有现有框架实现的深度学习程序
  - 生成：优化的 各种各样的硬件后端的 低层次的代码

- TVM面临的挑战：
  - 利用特定的硬件功能和抽象。
    - 输入的不同
    - 内存布局不同
    - 不同的、复杂的原语
    > 总结就是硬件多样
  - 依赖供应商提供的库
  - 需要同时兼顾计算图层次的优化和算子层次的优化
  - 大的算子优化搜索空间
    - 非人工的优化
    - 巨大的配置空间

- TVM的创新
  - 张量表达语言
  > 扩展了计算/调度分离概念。什么是计算调度分离概念？
  > 目标硬件元语从变换元语（调度元语）分离，以支持新的硬件（加速器）
  - 自动优化框架
  > 采用基于ML的代价模型
  - 图重写
  > 充分结合图级别的和算子级别的优化

- 贡献
  - 指出为不同硬件后端的深度学习工作流提供性能可移植性
  - 创新的利用跨线程内存复用、新硬件元语、延迟隐藏的调度元语
  - 自动探索和查找优化算子
  - 不同dl框架到硬件加速器的端到端的编译、优化栈

## TVM工作流

1. 从现有框架的模型转到计算图
2. 高层数据流重写生成优化的图
3. 算子层优化
    - 一组针对硬件的可能的优化
    - 基于ML的代价模型找到优化算子
4. 生成模型

![](imgs/tvm_fw.png)

### 计算图优化
- 节点表示算子，边表示数据依赖
- 图层次的优化：
  - 算子融合
  - 常量折叠
  - 静态内存规划
  - 数据布局转换

#### 算子融合
多个算子组合成一个算子，以避免往内存中存中间结果带来的开销
> 减少访存&计算 cost

- 四类图算子
  - 逐元素（injective）
  > 输出张量的元素只依赖输入张量的一个位置
  - 规约（reduction）
  > 输出张量的元素依赖输入张量的多个位置
  - 输出可融合复杂算子（complex-out-fusable）
  - 不透明算子（opaque，不可融合）

#### 数据布局转换

读数据通常是按行读，按列读，在DL中还会有如 4 $\times$ 4的形式需要4 $\tiems$ 4的数据块来优化本地访问

- 如何选择最优布局？
  - 算子对数据布局的偏好
  - 硬件特性
  - 全局图优化，尽量让多个算子采用同一布局，减少转换

### 张量算子生成

描述和计算规则解耦

### 自动优化






## TVM存在的问题和瓶颈

> answered by chatgpt

### 架构性瓶颈

### 自动优化瓶颈

### 性能瓶颈









<!--
# video: TVM  陈天奇：TVM 深度学习全栈自动优化和软硬件协同设计

- ai芯片设计
- 加速器是起点
  - 汇编
  - 支持上层的框架
  - 从头实现软件栈

- next gen dl fw
  - 全栈的优化
  - hard
    - 优于手写优化， 手写库 vs 编译器生成的库
    - 结合高层的优化和低层的优化

- tvm的优化
  - 计算图优化
    - 算子在计算图只只是一个节点，数据排布，精度等信息并没有被计算图所描述
  - 数学公式表达
-->

